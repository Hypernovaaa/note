# 梯度下降优化算法
- [参考文章](https://xinychen.github.io/books/spatiotemporal_low_rank_models.pdf)
- [知乎讲解](https://zhuanlan.zhihu.com/p/98642663)
## 求解目标
- 目标是求以下二次型的最小值
$$\begin{align}
f(x) = \frac{1}{2}x^T A x - b^Tx + c
\end{align}$$

- 由于系数矩阵A为对称矩阵，所以梯度为
$$\begin{align}
\nabla_x f(x) = \frac{1}{2} (A + A^T)x - b = Ax - b
\end{align}$$

- 这里令梯度值也就是一阶导数为0，就可以求得$f(x)$的极值，最优解的形式为
$$\begin{align}
x^* = A^{-1}b
\end{align}$$
- 当维度过高时，求解逆矩阵$A^{-1}$的代价非常大，计算复杂度为$O(n^3)$，并且大部分情况下不需要知道精确解，只需近似解即可满足需求，所以实际会使用下边的迭代求解方法
## 梯度下降法
- 首先有一组初始参数$x_0$，在这组初始参数$x_0$下求当前梯度$g_0 = A x_0 - b$，然后参数朝着负的梯度方向更新，使目标函数能够一步步走到最低点$x_1 = x_0 - \alpha g_0$，不断迭代更新直到求得能后使函数值最小的最优解$x^*$，更新公式可以归纳为
$$\begin{align}
\begin{cases}
g_t = A x_t - b \\
x_{t+1} = x_t - \alpha g_t
\end{cases}
\end{align}$$
## 最速梯度下降法
- 最速梯度下降法是在梯度下降法的基础上自动计算最优学习率$\alpha_t$，加速算法收敛，在一步更新中最优的学习率肯定是能够让函数值在这一步更新后最小的学习率，因此最优学习率可以定义为

$$\begin{align}
\hat{\alpha_t} &:= \argmin_{\alpha_t} f(x_{t+1}) \\
&= \argmin_{\alpha_t} f(x_t - \alpha_t g_t) \\
&= \argmin_{\alpha_t} \frac{1}{2}(x_t - \alpha_t g_t)^T A (x_t - \alpha_t g_t) - b^T(x_t - \alpha_t g_t) + c \\
&= \argmin_{\alpha_t} \frac{1}{2} \left[x_t^T A x_t - 
\underbrace{\alpha_t x_t^TAg_t - \alpha_t g_t^TA x_t}_{\text{转置合并}}
 + \alpha_t^2 g_t^T A g_t\right] - b^T x_t + b^T \alpha_t g_t + c \\
&= \argmin_{\alpha_t} \frac{1}{2} \alpha_t^2 g_t^T A g_t - \alpha_t g_t^T A x_t + \alpha_t g_t^T b + 
\underbrace{\frac{1}{2}x_t^T A x_t - b^T x_t + c}_{\text{无关的常数项}}
\end{align}$$
- 因为$g_t^T A x_t$是个标量，标量的转置还是本身，并且因为A是对称矩阵，所以$(g_t^T A x_t)^T = x_t^T A^T g_t = x_t^T A g_t$因此可以和前边合并
- 既然表达式有了那么想要求最小值，那么对学习率$\alpha_t$求偏导，$h$表示目标函数
$$\begin{align}
\frac{\partial h}{\partial \alpha_t} &= \alpha_t g_t^T A g_t - g_t^TA x_t + g_t^Tb \\
&= \alpha_t g_t^T A g_t - g_t^T(A x_t - b)
\end{align}$$

- 令偏导数为0可求得
$$\begin{align}
\alpha_t &= \frac{g_t^T(A x_t - b)}{g_t^T A g_t} \\
&= \frac{g_t^T g_t}{g_t^T A g_t}
\end{align}$$

- 最优梯度下降的迭代流程为
$$\begin{align}
\begin{cases}
g_t = Ax_t - b \\
\alpha_t = \frac{g_t^T g_t}{g_t^T A g_t} \\
x_{t+1} = x_t - \alpha_t g_t 
\end{cases}
\end{align}$$

- 对$\alpha_t$的偏导数为0可以推导出前后两步更新中的梯度方向是正交的
$$\begin{align}
\frac{\partial h}{\partial \alpha_t} &= a_t g_t^T A g_t - g_t^T(Ax_t -b) \\
&= -g_t^T(A x_t - b - \alpha_t A g_t) \\
&= -g_t^T(A(x_t - \alpha_t g_t) - b) \\
&= -g_t^T(A x_{t+1} - b)\\
&= -g_t^T g_{t+1} = 0
\end{align}$$
## 共轭梯度法
- 共轭向量的定义
    - 共轭向量是一组管局矩阵A正交的向量，普通向量正交需要满足$u^Tv = 0$，而共轭向量需要满足
    $$\begin{align}
    u^T A v = 0
    \end{align}$$

- 现在的目标还是求函数$f(x) = \frac{1}{2}x^TAx - bx + c$的最小值，函数的一阶导数为$\nabla_xf(x) = Ax - b$, 最优解为$x^*$,那么最优解应该满足$A x^* - b = 0$，如果$x^*$为N维向量，那么可以定义一组N维向量空间的基：$D = \{d_1, d_2, \cdots, d_N\}$这些向量基关于A共轭。注意这里的向量基和共轭条件都是定义出来的，当做已知量处理。那么这个最优解可以用这组基唯一表示
$$\begin{align}
x^* = \sum_{i=1}^N \alpha_i d_i
\end{align}$$

- 等式两边同时乘以$d_t^T A$，其中$d_t \in D$，第二个等号成立是因为根据共轭向量的定义，除了下标为t的向量都等于0消掉了
$$\begin{align}
d_t^T A x^* &= \sum_{i=1}^N \alpha_i d_t^T A d_i \\
&= \alpha_t d_t^T A d_t
\end{align}$$

- 可以推导出
$$\begin{align}
\alpha_t = \frac{d_t^T A x^*}{d_t^T A d_t} = \frac{d_t^T b}{d_t^TAd_t}
\end{align}$$

- 这里向量基$d_t \in D$是定义出来的相当于已知量，A 和 b是$f(x)$中的参数，也是已知量，那么$\alpha_t$可以求出来了，求出所有向量基底对应的参数就得到了精确解$x^*$，也就意味着求解最优解的优化步出为向量基的个数N，那么最后的问题就是怎么构造这一组共轭向量$D$

## 实际应用下的共轭梯度法
- 定义残差向量为梯度的反方向，指向了函数值下降最快的方向，那么用这个方向作为初始搜索方向再好不过了。$d_0 = r_0 = b - Ax_0$
$$\begin{align}
r_t = b - A x_t
\end{align}$$

- 公式21中是假定了向量基底已知的情况下最优解的表示形式，实际使用的时候是从一个初始化参数$x_0$出发，选定初始搜索方向之后$\alpha_0 d_0$是最优解在此搜索方向上的分量，因此以下更新规则表示在此搜索方向上朝着最优解走了一步
$$\begin{align}
x_{t+1} = x_t + \alpha_t d_t
\end{align}$$

- 残差向量之间相互正交$r_t^T r_t = 0$是作为条件给出的？


- 共轭梯度下降法的更新规则可以归纳为
$$\begin{align}
\begin{cases}
a_t = \frac{-d_t^T b}{d_t^T A d_t} \\
x_{t+1} = x_t + a_t d_t \\
\beta_t = \frac{}{}\\
d_{t+1} = r_{t+1} + \beta_t d_t
\end{cases}
\end{align}$$