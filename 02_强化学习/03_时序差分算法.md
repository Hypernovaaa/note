# 时序差分方法
- 不同于动态规划的假设，际应用的大多数情况下，状态转移函数和奖励函数是未知的，这种情况下智能体只能和环境进行交互，通过采样到的数据进行学习，这类学习方法统称为无模型的强化学习方法
- 时序差分方法是一种估计状态价值的方法
- 不同于蒙特卡洛方法，是采样一整条序列计算状态s的回报，采用期望的增量更新方式更更新状态s的价值，时序查分方法会估计下个状态的价值，这样在相邻两步之间就可以计算当前状态价值
$$ V(S_t) = V(S_t) + \alpha \left[r_t + \gamma V(s_{t+1}) - V(s_t) \right]$$
- 这里把增量更新公式中的$\frac{1}{N(s)}$直接替换成了常数$\alpha$，难道没有影响么？其实下一时刻的动作价值本身就是一个估计量，就算还是用$\frac{1}{N(s)}$准确更新状态价值的期望值也还是会有偏差，$\alpha$在这里可以理解为迭代求解过程中的学习率

# Sarsa算法
- 有了上边的状态价值函数的估计，离策略迭代算法就差一个策略提升了。首先策略用$\pi(a \mid s)$表示，表明了在状态s下采取行动a的概率分布。策略提升的目的是使新的策略在每个状态s后的动作价值都比原来的高，动态规划中的策略提升是依赖策略估计的状态价值函数的结果，新的策略是选择动作价值最高的动作，这里需要知道的是奖励函数和状态转移函数。
- 无模型的强化学习算法中，因为没有办法获取奖励函数和状态转移函数因此直接用时序查分算法估计动作价值函数
$$ 
Q(s_t, a_t) = Q(s_t, a_t) + \alpha[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$ 
- $\alpha$为已知量，$r_t$是环境给出的反馈也可以当做已知量，$Q(s_{t+1}, a_{t+1})$是下一时刻的动作价值，这里有点类似策略估计中的迭代求解，经过多轮迭代后动作价值和策略会逐渐收敛

# 多步Sarsa算法
- 蒙特卡洛方法计算某个状态的价值$V(s)$的时候使用的是历史访问到该状态的回报的平均值，都是确定的量因此是无偏估计，但是这种方式方差较大，不利于状态价值的准确估计
- Sarsa算法只用了前后两步的状动作价值，但是因为下一时刻的动作价值$Q(s_{t+1}, a_{t+1})$是一个估计量，因此是有偏估计
- 多步Sarsa算法综合了两者的优点，其实就是把回报展开式往前多推了几步
$$
Q(s_t, a_t) = Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + ... + \gamma^{n}Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]
$$
# Q-learning 算法
- Q-learnning和Sarsa的区别主要体现在了更新公式上
$$ 
Q(s_t, a_t) = Q(s_t, a_t) + \alpha[r_t + \gamma \max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)]
$$ 
- 可以将Q-learning类比动态规划中的价值迭代，都是基于贝尔曼最优方程
## 在线策略与离线策略
- 采样数据的策略称为行为策略，具体要优化的策略称为目标策略，在线策略和离线策略的区别主要体现在行为策略和目标策略是不是同一个策略，如果是同一个那么就是在线策略，如果不是同一个就是离线策略。如果行为策略和目标策略相同，明显此策略要再不断与环境交互的过程中更新，但是离线策略因为采样数据的策略和要更新的策略是两个，那么就可以先完成数据采样，再在采样的数据上更新目标策略。
- 以Q-learning为例，目标策略是一个完全贪婪策略，行为策略是一个$\epsilon$-贪婪策略，不是同一个因此这是离线策略。
- 离线策略的算法并不意味着可以不和环境进行交互，反而需要及时使用当前策略采样数据放入样本集中，保证训练数据和当前策略对应的数据分布有相似性。完全不和环境交互的算法是离线强化学习，离线强化学习$\neq$离线策略。
- 不使用$\epsilon$-贪心策略的Q-learning算法到底算不算在线策略？

# Dyna_Q算法
- 这是一种基于模型的强化学习算法，但是模型不是已知条件，是在智能体与环境交互的过程中记录下来的，在Q-learning的基础上每迭代一次，就在估计的模型中采样$N$条数据，使用这$N$条采样的数据更新模型。