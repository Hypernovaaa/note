# CartPole环境
- 之前处理的悬崖漫步和冰湖环境状态空间都是离散的，CartPole环境状态空间是连续的，动作空间是离散的
- 环境描述
![CartPole](https://i-blog.csdnimg.cn/direct/d9ad26e23e1f4033880191a2e02081a9.png)
# DQN算法
- 车杆环境相对悬崖漫步的环境最大的不同是状态空间由离散变为了连续，状态空间离散的情况下sarsa算法用贝尔曼期望方程计算动作价值，Q-learning用贝尔曼最优方程计算动作价值，DQN算法用神经网络拟合动作价值函数，这也是深度 Q 网络（deep Q network，DQN）算法名字的来历
- Q-learning的动作价值函数更新规则
$$\begin{align}
Q(s,a) = Q(s,a) + \alpha \left[r + \gamma \max_{a' \in A}Q(s', a') - Q(s,a)\right]
\end{align}$$
- Q-learning算法中,如果$r + \gamma \max_{a' \in A}Q(s', a')$ 和 $Q(s,a)$相近或者相等的话，动作价值函数$Q(s,a)$其实也就停止更新了，Q-learning在此时收敛，所以在DQN中设计如下均方误差损失函数，对于一组数据$\{(s_i, a_i, r_i, s_i')\}$
$$\begin{align}
\omega^* = \argmin_\omega \frac{1}{2N} \sum_{i=1}^N \left[Q_\omega(s_i, a_i) - \left(r_i + \gamma \max_{a'} Q_\omega(s'_i, a')\right)\right]^2
\end{align}$$
- $\omega$表示DQN动作价值估计网络，上式的意思是最优网络$\omega^*$是能够使后边损失项最小的网络模型
- 但是这里有个问题，一般情况下的均方误差为$L = \frac{1}{2N} \sum_{i}^N(y_i - \hat{y_i})^2$，这里的$y_i$是groundtruth，$\hat{y_i}$是预测值。类比公式2中的$Q_\omega(s_i, a_i)$ 和$\left(r_i + \gamma \max_{a'} Q_\omega(s'_i, a')\right)$这两项中的$Q_\omega$可都是模型的预测值，类似这种左脚踩右脚上天的功法我只知道武当派的梯云纵，显然这里不靠谱，解决方案是使用目标网络

## 目标网络
- 既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将时序差分目标中的 Q 网络固定住，因此需要两套Q网络
- 具体流程如下
	- 生成两个动作价值估计网络：待优化网络$Q_\omega(s,a)$和目标网络$Q_{\omega^-}(s,a)$，目标网络预测的动作价值当做groundtruth，训练的优化目标是让待优化网络逼近目标网络。
	- 待优化网络和目标网络都用默认方法初始化，待优化网络每轮迭代更新，目标网络每N轮和待优化网络同步参数
	- 迭代更新直到模型收敛
- 这里有两个问题（解答是个人理解）
	- 目标网络初始化后（并没有训练过）的推理输出肯定一点也不靠谱，那么为什么还可以当做优化目标呢？
		- DQN在采样动作的时候用了$\epsilon$-贪婪策略，目标网络获取动作价值的时候用的纯贪婪策略，虽然两个网络的输出都不靠谱，但是纯贪婪策略获取的动作价值更高，下等马亦有高下之分
		- 看真实值这一项$r_i + \gamma \max_{a'} Q_\omega(s'_i, a')$可不是只有目标网络的输出，在一个样本中如果待优化模型蒙对了动作，就会因为环境反馈奖励$r_i$强化这种动作，相反的如果蒙错了也会学习到去抑制这种错误操作。
	- 待优化网络和目标网络同步权重后，是不是预测值和真实值一样，没有梯度模型没法更新？
		- 预测值相当于是$Q_\omega(s_i, a_i)$，真实值是$r_i + \gamma \max_{a'} Q_\omega(s'_i, a')$，$a_i$是DQN算法采样的动作，$a'$是使动作价值最高的动作，这两个不一样，那么即使网络参数相同，获取的动作价值也是不同的。即使$Q_\omega(s_i, a_i)$和$\max_{a'} Q_\omega(s'_i, a')$的输出是一样的，后边这一项还有环境奖励$r_i$和折扣因子$\gamma$，也不会导致梯度为0的情况。

## 经验回放
- 维护一个回放缓冲区，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练
- 训练神经网络的一个基础假设是训练样本是独立同分布的IID，对一个采样序列来说，当前时刻的状态和上一个时刻的状态强相关，明显不满足独立同分布假设，将训练样本缓存相当于是训练数据有可能来自序列的不同时刻甚至是不同的序列，这样打破了样本之间的相关性，满足独立同分布假设，同时一个训练样本可以多次使用提高了样本使用效率

# DQN改进算法
- DQN算法存在Q值估值过高的问题
	- 估值过高的原因：
		- 优化目标$r_i + \gamma \max_{a'} Q_\omega(s'_i, a')$中的$\max_{a'}$表明我们想让动作价值尽量靠近DQN网络输出的最高动作价值。由于网络拟合的是估计值，总会有动作被估高了也会有动作被估计低了，再这样一堆估计值中选择最大值那么这个估计值整体来看是被高估的，因为最大值的期望$>$期望的最大值。
		- 用过高估计Q值的待优化网络去同步目标网络的权重，会让Q的估计越来越高，误差会累积。
	- 估值过高导致的问题：
		- 如果动作价值函数的估值过高会让模型倾向于选择更冒险的策略，或者会重复选择实际价值不高但是被高估了的动作，严重时会使策略不生效

## Double DQN算法
- Double DQN的解决方法是，用两个网络分别计算最优动作和最优动作的价值，比如存在两个计算动作价值的DQN网络A和B，从网络A中选择的最优动作a的动作价值$A(a)$大概率是被高估的，这时在网络B中计算动作a的动作价值$B(a)$有可能是被高估的也有可能是被低估的，高估和低估的不确定性相互抵消，总体来看用$B(a)$当做动作价值是更接近动作a的价值的期望的
- Double DQN的优化目标为
$$ r + \gamma Q_{\omega^-} \left(s', \argmax_{a'} Q_{\omega}(s',a')\right)$$
- 这里计算最优动作的网络为$Q_{\omega}$，计算动作价值的网络为$Q_{\omega^-}$

## Dueling DQN算法
- dueling有对决、对抗的意思，这里指的是不直接让网络输出动作的价值，而是输出状态价值$V(s)$和动作优势$A(s,a)$来共同决定动作价值，包含了两者的权衡所以用dueling这个词吧
- 在 Dueling DQN 中，Q 网络被建模为
$$
Q_{\eta,\alpha,\beta}(s,a) = V_{\eta, \alpha}(s) + A_{\eta, \beta}(s,a)
$$
	-	$V_{\eta, \alpha}(s)$是状态价值函数，$A_{\eta, \beta}(s,a)$是优势函数，代表了在当前状态s下采取不同动作之间的差异
	-	$\eta$是状态价值函数和优势函数中共享的网络参数，$\alpha$和$\beta$分别是两者的私有参数
- Q值分解的不唯一性：上式中的Q可以分解为V和A两项，对于任意常数c，$Q = (V + c) + (A - c)$都会成立，此时Q值的分解不唯一，会导致训练不稳定
- 最直观的解决方案是固定其中的一项就好了
$$
Q_{\eta,\alpha,\beta}(s,a) = V_{\eta, \alpha}(s) + A_{\eta, \beta}(s,a) - \max_{a'}A_{\eta, \beta}(s,a)
$$
	- 可见固定的是后边这一项，$A_{\eta, \beta}(s,a)$变成了$A_{\eta, \beta}(s,a) - \max_{a'}A_{\eta, \beta}(s,a)$	（以下为个人理解）	
		- $A_{\eta, \beta}(s,a)$的含义是状态s下执行动作a的优势，某个动作的优势可以拟合任意值，比如悬崖漫步的环境中，在状态20下对，如果网络想要学习到“向左走”的价值比“向右走”的价值高50这件事，动作网络可以输出任意的价值，比如（向左：100，向右50）或者（向左：50， 向右：0），这也是Q值分解不唯一为什么会训练不稳定
		- $A_{\eta, \beta}(s,a) - \max_{a'}A_{\eta, \beta}(s,a)$的含义是在状态s下执行动作a相对最优动作$a'$的优势差距有多少，这一项要想表达和上边相同的含义只能取值50
		- 可见经过替换后，可以理解为后边这一项由变量变为了常量，也就确保了Q值分解的唯一性。
- 实际应用的时候会使用平均替代最大化操作
$$
Q_{\eta,\alpha,\beta}(s,a) = V_{\eta, \alpha}(s) + A_{\eta, \beta}(s,a) -  \frac{1}{|\mathcal{A}|} \sum_{a'} A_{\eta, \beta}(s,a')
$$


