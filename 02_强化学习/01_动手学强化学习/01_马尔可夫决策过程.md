# 马尔可夫过程
- 马尔可夫过程可以用$\langle S,P \rangle$表示, S表示的是有限的状态空间合集，P表示状态转移矩阵,状态转移矩阵的第i行表示从状态$S_i$转移到别的状态的概率分布，所以状态转移矩阵的行和为1。
- 状态数量是有限的，并且状态转移矩阵是方阵
# 马尔可夫奖励过程
- 马尔可夫奖励过程用$\langle S,P,r,\gamma \rangle$表示
    - $r$ 表示奖励函数，指转移到该状态时获得奖励的期望，相当于多臂老虎机问题中每个摇杆的奖励期望，是个统计量不涉及状态转移的问题
    - $\gamma$ 表示折扣因子，在从当前时间往后看的采样路径的过程中，离当前时间获得的奖励权重大，越远离当前时间奖励的权重越小，这样设计是因为：长期的利益具有不确定性，因此更希望能尽快获得一些奖励

## 回报
- 从当前状态开始到终止状态（从终止状态无法转移到别的状态了）采样路径中，所有状态奖励的加权和
$$ \begin{equation} G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = 
\sum_{k=0}^\infty \gamma^k R_{t+k} \end{equation}$$

## 期望回报、价值
- 回报是采样了一条路径，通过该路径的奖励函数和折扣因子计算的该路径的回报
- 期望回报是从当前状态出发，不指定某路径，即从当前状态出发的的所有路径的回报的期望值，也叫做该状态的价值

## 价值函数
- 所有状态的价值就组成了价值函数（value function），价值函数的输入为某个状态,表示为$V(s) = \mathbb{E}[G_t \mid S_t = s]$
- 价值函数有一个递推关系, 这是后边编程的基础
$$V(s) = \mathbb{E}[R_t + \gamma V(S_t + 1) \mid S_t = s] $$
- 也就是当前时刻的价值是，E(初始状态的奖励 + $\gamma$ * 下一时刻的价值)
- 价值函数展开之后是$V(s) = r(s) + \gamma \sum_{s' \in S} p(s'\mid s) V(s')$

## 贝尔曼方程
- *递推关系就是贝尔曼方程，贝尔曼方程中未知量为每个状态的价值，已知量为状态转移矩阵、奖励函数、折扣因子(gpt)*
- 贝尔曼方程的表达式为
$$ V = R + \gamma P V $$
- 贝尔曼方程具有解析解,此时右边全部为已知量
$$ V = (I - \gamma P)^{-1}R $$
- 但是计算复杂度为$O(n^3)$，因此引出动态规划、蒙特卡洛和时序差分等方法

# 马尔可夫决策过程
- 马尔可夫奖励过程就像是一条小船在大海中随波飘荡的过程，马尔可夫决策过程则像是在船上放了一个水手控制着船前进，就可以主动决策并与环境交互。显然我们想要一个可以自主行动的模型
- 马尔可夫决策过程定义为$\langle S,A,P,r,\gamma \rangle$
    - $A$表示动作集和
    - $r(s,a)$是奖励函数，现在奖励不仅和状态有关还与动作有关
    - $P(s' \mid s,a)$表示状态转移函数
- 马尔可夫奖励过程中的奖励是和动作绑定的

## 策略
- 策略$ \pi(a \mid s) = P(A_t = a \mid S_t = s)$ 是一个关于状态的函数，表示在给定状态下可以执行的动作的概率，如果只能执行一个动作，则称为确定性策略否则称为概率性策略(*到底是只能执行一个策略还是说这个策略只能有一个目的地?*)

## 状态价值函数
- 类似马尔可夫奖励过程中的状态价值函数，只是加入了动作
$$ V^{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid S_t = s] $$

## 动作价值函数
- 在s状态下执行动作a的期望回报
$$ Q^{\pi}(s,a) = \mathbb{E}[G_t \mid S_t = s, A_t = a]$$
- 状态价值函数和动作价值函数之间的关系
$$ V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q^{\pi}(s,a)$$
$$ Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^{\pi}(s')$$

## 贝尔曼期望方程
- 贝尔曼期望方程是状态价值函数递推关系和动作价值函数递推关系组成的方程组
- 状态价值函数的递推关系
$$ V^{\pi}(s) = \sum_{a \in A}\pi(s,a)\left(r(s,a) + \gamma \sum_{s' \in S}P(s' \mid s,a)V^{\pi}(s') \right) $$ 

- 动作价值函数的递推关系
$$ Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s' \in S}P(s' \mid s, a) \sum_{a' \in A}\pi(a' \mid s')Q^{\pi}(s', a') $$

# 蒙特卡洛方法
- 采用抽样统计的方式来近似计算状态价值函数
$$ V^{\pi}(s) = \mathbb{E}[G_t \mid S_t = s] \approx \frac{1}{N} \sum_{i=1}^{N}G_t^{(i)}$$
- 从当前状态采样N条路径，计算回报求平均，表示当前状态的价值，对每个状态计算即可求得近似的价值函数
<p align="center"> <img src="https://jsd.cdn.zzko.cn/gh/Hypernovaaa/picx-images-hosting@master/20250815/image.2vf2i95hxi.jpg" width=600> </p>

# 占用度量
- 占用度量指的是在采样无限条无穷长路的路径中，每个状态被访问到的概率，表示为
$$ v^{\pi}(s) = (1 - \gamma) \sum_{t=0}^{\infty}\gamma^t p_t^{\pi}(s) $$
- 状态$s$的占用度量用$v^{\pi}(s)$表示，$p_t^{\pi}(s)$表示在策略$\pi$下在时间$t$访问到状态$s$的概率, 使用$\gamma^t$加权是为了符合折扣的思想，越是离当前时刻近就越重要。那么为什么要在前边加上$1- \gamma$呢？考虑一般情况下的占用度量公式
$$\begin{align}
 \sum_{s}v^{\pi}(s) &= (1-\gamma)\sum_{s} \sum_{t=0}^{\infty}\gamma^t p_t^{\pi}(s)\\
&= (1-\gamma)\sum_{t=0}^{\infty} \gamma^t \sum_{s}p_t^{\pi}(s) \\ 
&= (1-\gamma)\sum_{t=0}^{\infty}\gamma^t
\end{align}$$
- 到这里就很明显了，前边加$(1-\gamma)$这一项就是为了让占用度量是一个合法的概率分布公式，因为存在几何级数公式
$$ \sum_{t=0}^{\infty}\gamma^t = \frac{1}{1-\gamma}$$
- 状态访问概率具有如下性质
$$ v^{\pi}(s') = (1-\gamma)v_0(s') + \gamma \int P(s' \mid s,a) \pi(a \mid s)v^{\pi}(s)\,ds\,da $$
- 这里其实就是将占用度量中的0时刻和1时刻拆开了，$v_{0}(s')$表示的是0时刻的状态概率分布，前边这一项就没啥问题了，重点是后边这一项的$(1-\gamma)$因子哪里去了？后边这个表示的是从任意状态s出发到达状态$s'$的概率，因为对所有状态求积分并且是从时刻1开始的
$$ \int_{1}^{\infty} v^{\pi}(s)ds = \frac{\gamma}{1-\gamma}$$
- 因此前边的因子项变成了$\gamma$

## 扩展到策略的占用度量
$$ \rho^{\pi}(s,a) = (1-\gamma) \sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s) \pi(a \mid s)$$
- 两个相同的策略生成的占用度量必然是相同的，反之亦然
- 如果知道了某个状态动作对的访问概率，可以倒推出执行这个动作的概率，也是是说从占用度量可以反推出策略

# 最优策略
- 最优策略就是最好的策略用$\pi^{*}(s)$表示
- 最优状态价值函数
$$
V^{*}(s) = \max_{\pi}V^{\pi}(s),\;\forall s \in S
$$
- 最优动作价值函数
$$ 
Q^{*}(s,a) = \max_{\pi}Q^{\pi}(s,a) 
$$
# 最优贝尔曼方程
- 如果添加了最优的条件，那么在某状态s下执行的动作a是确定的
$$
V^{*}(s) = \max_{a \in A}\left\{r(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a)V^{*}(s')\right\}
$$ 
$$
Q^{*}(s,a) = r(s,a) + \gamma \sum_{s' \in S}p(s' \mid s, a) \max_{a' \in A}Q^{*}(s', a')
$$
- 贝尔曼期望方程的含义是我有一个确定的策略，基于贝尔曼期望方程可以得到状态价值函数和动作价值函数，贝尔曼最优方程的重点我觉得是，如果我想要最大化策略的状态价值函数和动作价值函数我应该采取什么策略