# 时序差分方法
- 不同于动态规划的假设，际应用的大多数情况下，状态转移函数和奖励函数是未知的，这种情况下智能体只能和环境进行交互，通过采样到的数据进行学习，这类学习方法统称为无模型的强化学习方法
- 时序差分方法是一种估计状态价值的方法
- 蒙特卡洛方法，是采样一整条序列计算状态s的回报，采用期望的增量更新方式更更新状态s的价值，时序差分方法会估计下个状态的价值，这样在相邻两步之间就可以计算当前状态价值

$$\begin{align}
V(s_t) &\leftarrow V(s_t) + \alpha [G_t - V(s_t)] \\
&\leftarrow V(s_t) + \alpha \left[r_t + \gamma V(s_{t+1}) - V(s_t) \right]
\end{align}$$

- 这里把增量更新公式中的$\frac{1}{N(s)}$直接替换成了常数$\alpha$，因为这里并没有完整采样一个序列, $\alpha$在这里可以理解为迭代求解过程中的学习率 

- 动态规划中的策略评估可以看做同时采样了多条无穷长的序列来估计状态价值, 时序差分可以看做采样无穷多的长度为2的序列, 两种方式最终都会收敛到状态价值函数

# Sarsa算法
- 有了上边的状态价值函数的估计，离策略迭代算法就差一个策略提升了, 那么如何在状态转移函数和奖励函数未知的情况下做策略提升?
- 首先策略用$\pi(a \mid s)$表示，表明了在状态s下采取行动a的概率分布。sarsa算法中直接估计动作价值函数, 可以用贪婪策略选择价值最大的动作作为最终策略
$$\begin{align} 
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
\end{align}$$ 
- $\alpha$为已知量，$r_t$是环境给出的反馈也可以当做已知量，$Q(s_{t+1}, a_{t+1})$是下一时刻的动作价值，这里有点类似策略估计中的迭代求解，经过多轮迭代后动作价值和策略会逐渐收敛

# 多步Sarsa算法
- 蒙特卡洛方法计算某个状态的价值$V(s)$的时候使用的是历史访问到该状态的回报的平均值，都是确定的量因此是无偏估计，但是这种方式方差较大，不利于状态价值的准确估计
- Sarsa算法只用了前后两步的状动作价值，但是因为下一时刻的动作价值$Q(s_{t+1}, a_{t+1})$是一个估计量，因此是有偏估计
- 多步Sarsa算法综合了两者的优点，其实就是把回报展开式往前多推了几步
$$
Q(s_t, a_t) = Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + ... + \gamma^{n}Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]
$$
# Q-learning 算法
- Q-learnning和Sarsa的区别主要体现在了更新公式上
$$ 
Q(s_t, a_t) = Q(s_t, a_t) + \alpha[r_t + \gamma \max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)]
$$ 
- 可以将Q-learning类比动态规划中的价值迭代，都是基于贝尔曼最优方程
## 在线策略与离线策略
- 采样数据的策略称为行为策略，具体要优化的策略称为目标策略，在线策略和离线策略的区别主要体现在行为策略和目标策略是不是同一个策略，如果是同一个那么就是在线策略，如果不是同一个就是离线策略。如果行为策略和目标策略相同，明显此策略要再不断与环境交互的过程中更新，但是离线策略因为采样数据的策略和要更新的策略是两个，那么就可以先完成数据采样，再在采样的数据上更新目标策略。
- 离线策略的算法并不意味着可以不和环境进行交互，反而需要及时使用当前策略采样数据放入样本集中，保证训练数据和当前策略对应的数据分布有相似性。完全不和环境交互的算法是离线强化学习，离线强化学习$\neq$离线策略。
<img src="https://i-blog.csdnimg.cn/direct/1c3cc063491a49a88cfd61355ac4cbbc.png" width=400>

# Dyna_Q算法
- 这是一种基于模型的强化学习算法，但是模型不是已知条件，是在智能体与环境交互的过程中记录下来的，在Q-learning的基础上每迭代一次，就在估计的模型中采样$N$条数据，使用这$N$条采样的数据更新模型。
- 基于模型的强化学习方法不一定要知道状态转移函数和奖励函数, Dyna_Q的建模过程就是使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态s，采取一个曾经在该状态下执行过的动作a，通过模型得到转移后的状态$s'$以及奖励r，并根据这个模拟数据$(s,a,r,s')$，用 Q-learning 的更新方式来更新动作价值函数

<img src="https://i-blog.csdnimg.cn/direct/98473c4467594e158d212ba12428b51d.png" width=400>