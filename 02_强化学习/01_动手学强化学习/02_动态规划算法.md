# 策略迭代
- 使用的局限性是，需要事先知道状态转移函数和奖励函数，并且马尔科夫奖励过程是有限的，即动作空间和状态空间是有限且离散的
- 动态规划的思想是将待求解问题分解成若干个子问题
## 实验环境
<img src="https://i-blog.csdnimg.cn/direct/709473a3c6a648dda1bf8e8115e0d750.png" alt="none" width=400>

- 它要求一个智能体从起点出发，避开悬崖行走，用尽可能少的步数到达目标位置, 如果遇到出口或者跌落悬崖则游戏结束

## 策略评估
- 策略评估指的是通过贝尔曼期望方程和一个初始策略，计算这个初始策略的状态价值函数
$$\begin{align}
V^{\pi}(s) = \sum_{a \in A} \pi(a|s)\left(r(s,a) + \gamma \sum_{s' \in S} p(s' |s,a) V^{\pi}(s')\right)
\end{align}$$

- 贝尔曼期望方程是用下一个时刻的状态价值估计当前时刻的状态价值, 因为都是同一个状态空间, 这里用上一个时刻的状态价值代替下一个时刻的状态价值来估计当前时刻的状态价值, 这里是可以证明收敛的. 比较直观的理解这里为什么收敛可以参考前边在蒙特卡洛方法中估计状态价值的过程, 可以当做我们是站在某个采样序列的末端估计策略的状态价值
- 实际在做策略评估的时候, 前后两步之间估计的状态价值的最大差值小于一定阈值则认为状态价值估计收敛了. 

## 策略提升 
- 如果对于每一个状态都存在一个动作$a'$，能够获得不低于原来的动作的价值，那么在每个状态$s$选择新的动作$a'$实际上就是一个新的策略，相对原来的策略有提升不就是策略提升么
- 策略迭代实际是一个从初始化策略，进行策略评估、策略提升、策略评估...不断迭代的过程，直到符合结束条件
- 当策略提升前后的价值一样时说明得到了最优的策略

# 价值迭代
- 贝尔曼最优方程
$$\begin{align}
V^*(s) = \max_{a \in A} \{r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) V^{k}(s') \}
\end{align}$$
- 在策略迭代算法中, 策略评估需要迭代很多轮才能收敛, 在动作空间和状态空间比较大的情况下需要很大的计算量. 而且完全存在这样的情况, 状态价值函数还没有收敛, 但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略. 价值迭代算法就是一种策略评估只进行了一轮更新的策略迭代算法

- 价值迭代实际上迭代的就是状态价值函数，基于贝尔曼最优方程，通过迭代求解方程求贝尔曼最优方程的驻点，最后通过求得的各个状态的价值还原最优策略