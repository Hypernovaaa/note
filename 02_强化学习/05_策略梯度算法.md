@[TOC]
# 基于策略的算法
- Q-learning以及DQN都是基于价值的方法，求得动作价值函数后就可以按照贪婪策略还原出一个策略。基于策略的方法会显式的学习一个目标策略，通常用一个网络模型表示，目标函数是初始状态的状态价值的期望，通过对策略求梯度使用梯度提升不断优化目标策略，使状态价值期望最大化。
- 这里关键的步骤是如何求策略的梯度

## 策略梯度
- 目标函数的定义
$$
\mathcal{J}(\theta) = \mathbb{E}_{s_0}[V^{\pi_\theta}(s_0)]
$$
	- $\pi_{\theta}$是一个随机性策略，$\theta$是对应的参数，目标函数$\mathcal{J}(\theta)$表示初始状态的状态价值的期望
	- $\mathbb{E}_{s_0}[]$中的下标的含义是对随机变量$s_0$的分布求期望
- 对目标函数求梯度
$$\begin{align}
\nabla_{\theta}\mathcal{J}(\theta) &\propto \sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A}Q^{\pi_\theta}(s,a) \nabla_{\theta} \pi_\theta(a \mid s) \\
&=\sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A} \pi_\theta(a \mid s) Q^{\pi_\theta}(s,a) \frac{\nabla_\theta \pi_\theta(a \mid s)}{\pi_\theta(a \mid s)} \\
&= \mathbb{E}_{\pi_\theta}\left[Q^{\pi_\theta}(s,a) \nabla_\theta \log \pi_\theta(a \mid s)\right]
\end{align}$$
	- $\nu^{\pi_\theta}(s)$表示在策略$\pi_\theta$下各个状态的占用度量，也就是各个状态被访问到的概率，这里可以当做一个已知量，实际是根据采样结果近似，在优化目标公式3中已经没有这一项了，所以是否知道占用度量也无所谓了？
	- $Q^{\pi_\theta}(s,a)$表示在策略$\pi_\theta$下动作价值函数，$\nabla_\theta \log \pi_\theta(a \mid s)$是策略模型的梯度，策略是网络模型这里相当于神经网络的梯度，算是已知项，所以只要知道动作价值就可以求出目标函数的梯度
		- REINFORCE中使用蒙特卡洛方法估计$Q^{\pi_\theta}(s,a)$
$$\begin{align}
\nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} \left[\sum_{t=0}^T \left(\sum_{t'=t}^T \gamma^{t'-t} r_{t'}\right) \nabla_\theta \log \pi_\theta(a_t \mid s_t)\right] 
\end{align}$$
				- T是和环境最大交互步数
				- Q-learning和DQN中的动作价值函数直接用的模型输出结果，那是因为优化目标就是动作价值函数，可以通过不断迭代使动作价值函数收敛，在基于策略的算法中没有对价值函数的迭代过程，也没有输出动作价值的模型。
	- 公式2到公式3是根据：
$$
\frac{d}{dx} \log f(x) = \frac{1}{f(x)} \frac{df(x)}{dx}
$$
- REINFORCE算法流程
	- 初始化策略参数$\theta$
	- for 序列 $e = 1 \rightarrow E$ do:
		- 1、用当前策略$\pi_\theta$采样轨迹$\{s_1, a_1, r_1, s_2, a_2, r_2,...,s_T, a_T, r_T\}$
		- 2、计算当前轨迹每个时刻t往后的回报$\sum_{t'=t}^T \gamma^{t'-t}r_{t'}$，记为$\psi_t$
		- 3、对$\theta$进行更新，$\theta = \theta + \alpha \sum_t^T \psi_t \nabla_\theta \log \pi_\theta(a_t \mid s_t)$
	- end for
	```python
	        for i in reversed(range(len(reward_list))):  # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]],
                                 dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = self.gamma * G + reward
            loss = -log_prob * G  # 每一步的损失函数
            loss.backward()  # 反向传播计算梯度
        self.optimizer.step()  # 梯度下降
	```
	- 上述代码是对一条序列的学习过程，每条序列模型只更新一次，代码里log_prob前边的负号是因为torch默认最小化损失，我们想最大化价值期望。
## 目标函数的推导