# 梯度下降优化算法
[参考文章](https://xinychen.github.io/books/spatiotemporal_low_rank_models.pdf)
## 求解目标
- 目标是求以下二次型的最小值
$$\begin{align}
f(x) = \frac{1}{2}x^T A x - b^Tx + c
\end{align}$$

- 由于系数矩阵A为对称矩阵，所以梯度为
$$\begin{align}
\nabla_x f(x) = \frac{1}{2} (A + A^T)x - b = Ax - b
\end{align}$$

- 这里令梯度值也就是一阶导数为0，就可以求得$f(x)$的极值，最优解的形式为
$$\begin{align}
x^* = A^{-1}b
\end{align}$$
- 当维度过高时，求解逆矩阵$A^{-1}$的代价非常大，计算复杂度为$O(n^3)$，并且大部分情况下不需要知道精确解，只需近似解即可满足需求，所以实际会使用下边的迭代求解方法
## 梯度下降法
- 首先有一组初始参数$x_0$，在这组初始参数$x_0$下求当前梯度$g_0 = A x_0 - b$，然后参数朝着负的梯度方向更新，使目标函数能够一步步走到最低点$x_1 = x_0 - \alpha g_0$，不断迭代更新直到求得能后使函数值最小的最优解$x^*$，更新公式可以归纳为
$$\begin{align}
\begin{cases}
g_t = A x_t - b \\
x_{t+1} = x_t - \alpha g_t
\end{cases}
\end{align}$$
## 最速梯度下降法
- 最速梯度下降法是在梯度下降法的基础上自动计算最优学习率$\alpha_t$，加速算法收敛，在一步更新中最优的学习率肯定是能够让函数值在这一步更新后最小的学习率，因此最优学习率可以定义为
$$\begin{align}
\hat{\alpha_t} &:= \argmin_{\alpha_t} f(x_{t+1}) \\
&= \argmin_{\alpha_t} f(x_t - \alpha_t g_t) \\
&= \argmin_{\alpha_t} \frac{1}{2}(x_t - \alpha_t g_t)^T A (x_t - \alpha_t g_t) - b^T(x_t - \alpha_t g_t) + c \\
&= \argmin_{\alpha_t} \frac{1}{2} \left[x_t^T A x_t - 
\underbrace{\alpha_t x_t^TAg_t - \alpha_t g_t^TA x_t}_{\text{转置合并}}
 + \alpha_t^2 g_t^T A g_t\right] - b^T x_t + b^T \alpha_t g_t + c \\
&= \argmin_{\alpha_t} \frac{1}{2} \alpha_t^2 g_t^T A g_t - \alpha_t g_t^T A x_t + \alpha_t g_t^T b + 
\underbrace{\frac{1}{2}x_t^T A x_t - b^T x_t + c}_{\text{无关的常数项}}
\end{align}$$
- 因为$g_t^T A x_t$是个标量，标量的转置还是本身，并且因为A是对称矩阵，所以$(g_t^T A x_t)^T = x_t^T A^T g_t = x_t^T A g_t$因此可以和前边合并
- 既然表达式有了那么想要求最小值，那么对学习率$\alpha_t$求偏导，$h$表示目标函数
$$\begin{align}
\frac{\partial h}{\partial \alpha_t} &= \alpha_t g_t^T A g_t - g_t^TA x_t + g_t^Tb \\
&= \alpha_t g_t^T A g_t - g_t^T(A x_t - b)
\end{align}$$

- 令偏导数为0可求得
$$\begin{align}
\alpha_t &= \frac{g_t^T(A x_t - b)}{g_t^T A g_t} \\
&= \frac{g_t^T g_t}{g_t^T A g_t}
\end{align}$$

- 最优梯度下降的迭代流程为
$$\begin{align}
\begin{cases}
g_t &= Ax_t - b \\
\alpha_t &= \frac{g_t^T g_t}{g_t^T A g_t} \\
x_{t+1} &= x_t - \alpha_t g_t 
\end{cases}
\end{align}$$

## 共轭梯度法
- 共轭向量是一组管局矩阵A正交的向量，普通向量正交需要满足$u^Tv = 0$，而共轭向量需要满足
$$\begin{align}
u^T A v = 0
\end{align}$$
